import torch
import torch.nn as nn
from einops import repeat

class Expand(nn.Module):
    """
    Expand fixed-length representation into variable-length sequence embedding.

    Takes the fixed-length representations generated by Reduce and expands them
    back into a specified sequence length. Takes the same mean values and just
    repeats them until there are the length of the desired sequence.
    """
    def __init__(self):
        super(Expand, self).__init__()

    def forward(self, encoding, target_length):
        """
        Return variable-length encoding according to target_length using
        fixed-length input embedding.
        """
        batch_size, num_segments, _ = encoding.shape
        
        # calculate the number of segment repetitions needed to bring encoding to target_length
        num_repetitions = target_length // num_segments
        num_gap_segments = target_length % num_segments

        # some positions left over when target_length is not evenly divisble by num_segments
        # repeat more at beginning and end to fill in the gap
        left_length = num_gap_segments // 2 + num_gap_segments % 2
        right_length = num_gap_segments // 2
        middle_length = num_segments - num_gap_segments

        # repeat each segment
        repetitions = []

        # repeat left
        if left_length > 0:
            encoding_left = encoding[:, :left_length, :]
            repeated_left = repeat(encoding_left, "b h c -> b (h rep) c", rep=num_repetitions + 1)
            repetitions.append(repeated_left)

        # repeat middle
        encoding_middle = encoding[:, left_length:(left_length + middle_length), :]
        repeated_middle = repeat(encoding_middle, "b h c -> b (h rep) c", rep=num_repetitions)
        repetitions.append(repeated_middle)

        # repeat right 
        if right_length > 0:
             enc_right = encoding[:, num_segments - right_length:, :]
             repeated_right = repeat(enc_right, f"b h c -> b (h rep) c", rep=num_repetitions + 1)
             repetitions.append(repeated_right)

        # concatentate constructed embeddings
        constructed_embedding = torch.cat(repetitions, dim=1)

        return constructed_embedding









