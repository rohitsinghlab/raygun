# Copyright 2024  Kapil Devkota, Rohit Singh
# All rights reserved
# This code is available under the terms of the license available at https://github.com/rohitsinghlab/raygun
import torch.nn as nn
import torch
from einops import repeat # Only 'repeat' is used from einops

class Repitition(nn.Module):
    """
    Expand fixed-length representation into variable-length sequence embedding.

    Takes the fixed-length representations (e.g., generated by a reduction module) 
    and expands them back into a specified sequence length. Segments of the input
    encoding are repeated to achieve the target length.
    """
    def __init__(self):
        """
        Initializes the Repitition module.
        """
        super(Repitition, self).__init__()
        # Original file had 'return' here, which is not necessary for __init__
        
    def forward(self, encoding: torch.Tensor, finallength: int) -> torch.Tensor:
        """
        Return variable-length encoding according to finallength using
        fixed-length input embedding (encoding).
        """
        batch_size, encoderlength, dim = encoding.shape # Unpack dimensions
        
        # calculate the base number of segment repetitions needed
        reps = finallength // encoderlength
        # calculate remaining length (gap) if finallength is not evenly divisible by encoderlength
        gap  = finallength % encoderlength
        
        # distribute the gap: segments at the beginning and end will be repeated one extra time
        # to fill in the gap
        gapleft = gapright = gap // 2
        if gap % 2 == 1: # if gap is odd, assign the extra repetition to the left side
            gapleft += 1
        
        # number of segments in the middle part that will be repeated 'reps' times
        mid = encoderlength - gap 
        
        # Process left part
        if gapleft == 0:
            xstart = torch.zeros(batch_size, 0, dim).to(encoding.device)
        else:
            # select the first 'gapleft' segments from encoding
            encstart = encoding[:, :gapleft, :]
            # repeat left segments
            xstart = repeat(encstart, f"b h c -> b (h rep) c", rep=reps+1)
            
        # Process middle part
        # select 'mid' segments from encoding
        encmid = encoding[:, gapleft:(gapleft + mid), :]
        if mid == 0 : 
            xmid = torch.zeros(batch_size, 0, dim).to(encoding.device)
        else:
            # repeat middle segments
            xmid = repeat(encmid, f"b h c -> b (h rep) c", rep = reps)
            
        # Process right part
        if gapright == 0:
            xend = torch.zeros(batch_size, 0, dim).to(encoding.device)
        else:
            # select the last 'gapright' segments from encoding
            encend = encoding[:, (gapleft + mid):, :] 
            # repeat right segments
            xend = repeat(encend, f"b h c -> b (h rep) c", rep = reps+1)
            
        # concatenate constructed embeddings
        return torch.concat([xstart, xmid, xend], dim = 1)
