{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raygun.modelv3.raygun import Raygun\n",
    "from raygun.modelv3.ltraygun import RaygunLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/hpc/group/singhlab/rawdata/raygunv2models/species_function_aware_sep30_val_blosum_0.9856.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.pretrained import esm2_t33_650M_UR50D\n",
    "\n",
    "emodel, ealph = esm2_t33_650M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raymodel = Raygun(numdecoders=12, \n",
    "                 numencoders=12, \n",
    "                 fixed_esm_batching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __init__(self, raygun, esmmodel,\n",
      "                lr = 1e-3, \n",
      "                crossentropyloss = 1., \n",
      "                reconstructionloss = 1., \n",
      "                replicateloss = 1.,\n",
      "                log_wandb = False,\n",
      "                traininglog = \"traininglog.txt\",\n",
      "                finetune = False):\n",
      "        super().__init__()\n",
      "        self.model            = raygun\n",
      "        self.esmmodel         = esmmodel\n",
      "        self.lr               = lr\n",
      "        self.crossentropyloss = crossentropyloss\n",
      "        self.reconstructloss  = reconstructionloss\n",
      "        self.replicateloss    = replicateloss\n",
      "        self.trainlosses      = defaultdict(list)\n",
      "        self.vallosses        = defaultdict(list)\n",
      "        self.epoch            = 0\n",
      "        bl                    = substitution_matrices.load(\"BLOSUM62\")\n",
      "        self.blosummat        = pd.DataFrame(bl, columns = list(bl.alphabet))\n",
      "        self.blosummat.index  = list(bl.alphabet)\n",
      "        self.decodermodel     = raygun.esmdecoder\n",
      "        \n",
      "        self.esmalphabet      = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, \n",
      "                                 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, \n",
      "                                 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, \n",
      "                                 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
      "        self.toktoalphdict    = {k: i for i, k in self.esmalphabet.items()} \n",
      "        \n",
      "        self.log_wandb        = log_wandb\n",
      "        self.traininglog      = traininglog\n",
      "        \n",
      "        # loss regularization\n",
      "        self.runid            = 0\n",
      "        self.tlosshistory     = []\n",
      "        self.coolingtime      = 100\n",
      "        self.averagingwindow  = 500\n",
      "        self.std_threshold    = 15\n",
      "        self.finetune         = finetune\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(RaygunLightning.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['esmmodel.embed_tokens.weight', 'esmmodel.layers.0.self_attn.k_proj.weight', 'esmmodel.layers.0.self_attn.k_proj.bias', 'esmmodel.layers.0.self_attn.v_proj.weight', 'esmmodel.layers.0.self_attn.v_proj.bias', 'esmmodel.layers.0.self_attn.q_proj.weight', 'esmmodel.layers.0.self_attn.q_proj.bias', 'esmmodel.layers.0.self_attn.out_proj.weight', 'esmmodel.layers.0.self_attn.out_proj.bias', 'esmmodel.layers.0.self_attn.rot_emb.inv_freq', 'esmmodel.layers.0.self_attn_layer_norm.weight', 'esmmodel.layers.0.self_attn_layer_norm.bias', 'esmmodel.layers.0.fc1.weight', 'esmmodel.layers.0.fc1.bias', 'esmmodel.layers.0.fc2.weight', 'esmmodel.layers.0.fc2.bias', 'esmmodel.layers.0.final_layer_norm.weight', 'esmmodel.layers.0.final_layer_norm.bias', 'esmmodel.layers.1.self_attn.k_proj.weight', 'esmmodel.layers.1.self_attn.k_proj.bias', 'esmmodel.layers.1.self_attn.v_proj.weight', 'esmmodel.layers.1.self_attn.v_proj.bias', 'esmmodel.layers.1.self_attn.q_proj.weight', 'esmmodel.layers.1.self_attn.q_proj.bias', 'esmmodel.layers.1.self_attn.out_proj.weight', 'esmmodel.layers.1.self_attn.out_proj.bias', 'esmmodel.layers.1.self_attn.rot_emb.inv_freq', 'esmmodel.layers.1.self_attn_layer_norm.weight', 'esmmodel.layers.1.self_attn_layer_norm.bias', 'esmmodel.layers.1.fc1.weight', 'esmmodel.layers.1.fc1.bias', 'esmmodel.layers.1.fc2.weight', 'esmmodel.layers.1.fc2.bias', 'esmmodel.layers.1.final_layer_norm.weight', 'esmmodel.layers.1.final_layer_norm.bias', 'esmmodel.layers.2.self_attn.k_proj.weight', 'esmmodel.layers.2.self_attn.k_proj.bias', 'esmmodel.layers.2.self_attn.v_proj.weight', 'esmmodel.layers.2.self_attn.v_proj.bias', 'esmmodel.layers.2.self_attn.q_proj.weight', 'esmmodel.layers.2.self_attn.q_proj.bias', 'esmmodel.layers.2.self_attn.out_proj.weight', 'esmmodel.layers.2.self_attn.out_proj.bias', 'esmmodel.layers.2.self_attn.rot_emb.inv_freq', 'esmmodel.layers.2.self_attn_layer_norm.weight', 'esmmodel.layers.2.self_attn_layer_norm.bias', 'esmmodel.layers.2.fc1.weight', 'esmmodel.layers.2.fc1.bias', 'esmmodel.layers.2.fc2.weight', 'esmmodel.layers.2.fc2.bias', 'esmmodel.layers.2.final_layer_norm.weight', 'esmmodel.layers.2.final_layer_norm.bias', 'esmmodel.layers.3.self_attn.k_proj.weight', 'esmmodel.layers.3.self_attn.k_proj.bias', 'esmmodel.layers.3.self_attn.v_proj.weight', 'esmmodel.layers.3.self_attn.v_proj.bias', 'esmmodel.layers.3.self_attn.q_proj.weight', 'esmmodel.layers.3.self_attn.q_proj.bias', 'esmmodel.layers.3.self_attn.out_proj.weight', 'esmmodel.layers.3.self_attn.out_proj.bias', 'esmmodel.layers.3.self_attn.rot_emb.inv_freq', 'esmmodel.layers.3.self_attn_layer_norm.weight', 'esmmodel.layers.3.self_attn_layer_norm.bias', 'esmmodel.layers.3.fc1.weight', 'esmmodel.layers.3.fc1.bias', 'esmmodel.layers.3.fc2.weight', 'esmmodel.layers.3.fc2.bias', 'esmmodel.layers.3.final_layer_norm.weight', 'esmmodel.layers.3.final_layer_norm.bias', 'esmmodel.layers.4.self_attn.k_proj.weight', 'esmmodel.layers.4.self_attn.k_proj.bias', 'esmmodel.layers.4.self_attn.v_proj.weight', 'esmmodel.layers.4.self_attn.v_proj.bias', 'esmmodel.layers.4.self_attn.q_proj.weight', 'esmmodel.layers.4.self_attn.q_proj.bias', 'esmmodel.layers.4.self_attn.out_proj.weight', 'esmmodel.layers.4.self_attn.out_proj.bias', 'esmmodel.layers.4.self_attn.rot_emb.inv_freq', 'esmmodel.layers.4.self_attn_layer_norm.weight', 'esmmodel.layers.4.self_attn_layer_norm.bias', 'esmmodel.layers.4.fc1.weight', 'esmmodel.layers.4.fc1.bias', 'esmmodel.layers.4.fc2.weight', 'esmmodel.layers.4.fc2.bias', 'esmmodel.layers.4.final_layer_norm.weight', 'esmmodel.layers.4.final_layer_norm.bias', 'esmmodel.layers.5.self_attn.k_proj.weight', 'esmmodel.layers.5.self_attn.k_proj.bias', 'esmmodel.layers.5.self_attn.v_proj.weight', 'esmmodel.layers.5.self_attn.v_proj.bias', 'esmmodel.layers.5.self_attn.q_proj.weight', 'esmmodel.layers.5.self_attn.q_proj.bias', 'esmmodel.layers.5.self_attn.out_proj.weight', 'esmmodel.layers.5.self_attn.out_proj.bias', 'esmmodel.layers.5.self_attn.rot_emb.inv_freq', 'esmmodel.layers.5.self_attn_layer_norm.weight', 'esmmodel.layers.5.self_attn_layer_norm.bias', 'esmmodel.layers.5.fc1.weight', 'esmmodel.layers.5.fc1.bias', 'esmmodel.layers.5.fc2.weight', 'esmmodel.layers.5.fc2.bias', 'esmmodel.layers.5.final_layer_norm.weight', 'esmmodel.layers.5.final_layer_norm.bias', 'esmmodel.layers.6.self_attn.k_proj.weight', 'esmmodel.layers.6.self_attn.k_proj.bias', 'esmmodel.layers.6.self_attn.v_proj.weight', 'esmmodel.layers.6.self_attn.v_proj.bias', 'esmmodel.layers.6.self_attn.q_proj.weight', 'esmmodel.layers.6.self_attn.q_proj.bias', 'esmmodel.layers.6.self_attn.out_proj.weight', 'esmmodel.layers.6.self_attn.out_proj.bias', 'esmmodel.layers.6.self_attn.rot_emb.inv_freq', 'esmmodel.layers.6.self_attn_layer_norm.weight', 'esmmodel.layers.6.self_attn_layer_norm.bias', 'esmmodel.layers.6.fc1.weight', 'esmmodel.layers.6.fc1.bias', 'esmmodel.layers.6.fc2.weight', 'esmmodel.layers.6.fc2.bias', 'esmmodel.layers.6.final_layer_norm.weight', 'esmmodel.layers.6.final_layer_norm.bias', 'esmmodel.layers.7.self_attn.k_proj.weight', 'esmmodel.layers.7.self_attn.k_proj.bias', 'esmmodel.layers.7.self_attn.v_proj.weight', 'esmmodel.layers.7.self_attn.v_proj.bias', 'esmmodel.layers.7.self_attn.q_proj.weight', 'esmmodel.layers.7.self_attn.q_proj.bias', 'esmmodel.layers.7.self_attn.out_proj.weight', 'esmmodel.layers.7.self_attn.out_proj.bias', 'esmmodel.layers.7.self_attn.rot_emb.inv_freq', 'esmmodel.layers.7.self_attn_layer_norm.weight', 'esmmodel.layers.7.self_attn_layer_norm.bias', 'esmmodel.layers.7.fc1.weight', 'esmmodel.layers.7.fc1.bias', 'esmmodel.layers.7.fc2.weight', 'esmmodel.layers.7.fc2.bias', 'esmmodel.layers.7.final_layer_norm.weight', 'esmmodel.layers.7.final_layer_norm.bias', 'esmmodel.layers.8.self_attn.k_proj.weight', 'esmmodel.layers.8.self_attn.k_proj.bias', 'esmmodel.layers.8.self_attn.v_proj.weight', 'esmmodel.layers.8.self_attn.v_proj.bias', 'esmmodel.layers.8.self_attn.q_proj.weight', 'esmmodel.layers.8.self_attn.q_proj.bias', 'esmmodel.layers.8.self_attn.out_proj.weight', 'esmmodel.layers.8.self_attn.out_proj.bias', 'esmmodel.layers.8.self_attn.rot_emb.inv_freq', 'esmmodel.layers.8.self_attn_layer_norm.weight', 'esmmodel.layers.8.self_attn_layer_norm.bias', 'esmmodel.layers.8.fc1.weight', 'esmmodel.layers.8.fc1.bias', 'esmmodel.layers.8.fc2.weight', 'esmmodel.layers.8.fc2.bias', 'esmmodel.layers.8.final_layer_norm.weight', 'esmmodel.layers.8.final_layer_norm.bias', 'esmmodel.layers.9.self_attn.k_proj.weight', 'esmmodel.layers.9.self_attn.k_proj.bias', 'esmmodel.layers.9.self_attn.v_proj.weight', 'esmmodel.layers.9.self_attn.v_proj.bias', 'esmmodel.layers.9.self_attn.q_proj.weight', 'esmmodel.layers.9.self_attn.q_proj.bias', 'esmmodel.layers.9.self_attn.out_proj.weight', 'esmmodel.layers.9.self_attn.out_proj.bias', 'esmmodel.layers.9.self_attn.rot_emb.inv_freq', 'esmmodel.layers.9.self_attn_layer_norm.weight', 'esmmodel.layers.9.self_attn_layer_norm.bias', 'esmmodel.layers.9.fc1.weight', 'esmmodel.layers.9.fc1.bias', 'esmmodel.layers.9.fc2.weight', 'esmmodel.layers.9.fc2.bias', 'esmmodel.layers.9.final_layer_norm.weight', 'esmmodel.layers.9.final_layer_norm.bias', 'esmmodel.layers.10.self_attn.k_proj.weight', 'esmmodel.layers.10.self_attn.k_proj.bias', 'esmmodel.layers.10.self_attn.v_proj.weight', 'esmmodel.layers.10.self_attn.v_proj.bias', 'esmmodel.layers.10.self_attn.q_proj.weight', 'esmmodel.layers.10.self_attn.q_proj.bias', 'esmmodel.layers.10.self_attn.out_proj.weight', 'esmmodel.layers.10.self_attn.out_proj.bias', 'esmmodel.layers.10.self_attn.rot_emb.inv_freq', 'esmmodel.layers.10.self_attn_layer_norm.weight', 'esmmodel.layers.10.self_attn_layer_norm.bias', 'esmmodel.layers.10.fc1.weight', 'esmmodel.layers.10.fc1.bias', 'esmmodel.layers.10.fc2.weight', 'esmmodel.layers.10.fc2.bias', 'esmmodel.layers.10.final_layer_norm.weight', 'esmmodel.layers.10.final_layer_norm.bias', 'esmmodel.layers.11.self_attn.k_proj.weight', 'esmmodel.layers.11.self_attn.k_proj.bias', 'esmmodel.layers.11.self_attn.v_proj.weight', 'esmmodel.layers.11.self_attn.v_proj.bias', 'esmmodel.layers.11.self_attn.q_proj.weight', 'esmmodel.layers.11.self_attn.q_proj.bias', 'esmmodel.layers.11.self_attn.out_proj.weight', 'esmmodel.layers.11.self_attn.out_proj.bias', 'esmmodel.layers.11.self_attn.rot_emb.inv_freq', 'esmmodel.layers.11.self_attn_layer_norm.weight', 'esmmodel.layers.11.self_attn_layer_norm.bias', 'esmmodel.layers.11.fc1.weight', 'esmmodel.layers.11.fc1.bias', 'esmmodel.layers.11.fc2.weight', 'esmmodel.layers.11.fc2.bias', 'esmmodel.layers.11.final_layer_norm.weight', 'esmmodel.layers.11.final_layer_norm.bias', 'esmmodel.layers.12.self_attn.k_proj.weight', 'esmmodel.layers.12.self_attn.k_proj.bias', 'esmmodel.layers.12.self_attn.v_proj.weight', 'esmmodel.layers.12.self_attn.v_proj.bias', 'esmmodel.layers.12.self_attn.q_proj.weight', 'esmmodel.layers.12.self_attn.q_proj.bias', 'esmmodel.layers.12.self_attn.out_proj.weight', 'esmmodel.layers.12.self_attn.out_proj.bias', 'esmmodel.layers.12.self_attn.rot_emb.inv_freq', 'esmmodel.layers.12.self_attn_layer_norm.weight', 'esmmodel.layers.12.self_attn_layer_norm.bias', 'esmmodel.layers.12.fc1.weight', 'esmmodel.layers.12.fc1.bias', 'esmmodel.layers.12.fc2.weight', 'esmmodel.layers.12.fc2.bias', 'esmmodel.layers.12.final_layer_norm.weight', 'esmmodel.layers.12.final_layer_norm.bias', 'esmmodel.layers.13.self_attn.k_proj.weight', 'esmmodel.layers.13.self_attn.k_proj.bias', 'esmmodel.layers.13.self_attn.v_proj.weight', 'esmmodel.layers.13.self_attn.v_proj.bias', 'esmmodel.layers.13.self_attn.q_proj.weight', 'esmmodel.layers.13.self_attn.q_proj.bias', 'esmmodel.layers.13.self_attn.out_proj.weight', 'esmmodel.layers.13.self_attn.out_proj.bias', 'esmmodel.layers.13.self_attn.rot_emb.inv_freq', 'esmmodel.layers.13.self_attn_layer_norm.weight', 'esmmodel.layers.13.self_attn_layer_norm.bias', 'esmmodel.layers.13.fc1.weight', 'esmmodel.layers.13.fc1.bias', 'esmmodel.layers.13.fc2.weight', 'esmmodel.layers.13.fc2.bias', 'esmmodel.layers.13.final_layer_norm.weight', 'esmmodel.layers.13.final_layer_norm.bias', 'esmmodel.layers.14.self_attn.k_proj.weight', 'esmmodel.layers.14.self_attn.k_proj.bias', 'esmmodel.layers.14.self_attn.v_proj.weight', 'esmmodel.layers.14.self_attn.v_proj.bias', 'esmmodel.layers.14.self_attn.q_proj.weight', 'esmmodel.layers.14.self_attn.q_proj.bias', 'esmmodel.layers.14.self_attn.out_proj.weight', 'esmmodel.layers.14.self_attn.out_proj.bias', 'esmmodel.layers.14.self_attn.rot_emb.inv_freq', 'esmmodel.layers.14.self_attn_layer_norm.weight', 'esmmodel.layers.14.self_attn_layer_norm.bias', 'esmmodel.layers.14.fc1.weight', 'esmmodel.layers.14.fc1.bias', 'esmmodel.layers.14.fc2.weight', 'esmmodel.layers.14.fc2.bias', 'esmmodel.layers.14.final_layer_norm.weight', 'esmmodel.layers.14.final_layer_norm.bias', 'esmmodel.layers.15.self_attn.k_proj.weight', 'esmmodel.layers.15.self_attn.k_proj.bias', 'esmmodel.layers.15.self_attn.v_proj.weight', 'esmmodel.layers.15.self_attn.v_proj.bias', 'esmmodel.layers.15.self_attn.q_proj.weight', 'esmmodel.layers.15.self_attn.q_proj.bias', 'esmmodel.layers.15.self_attn.out_proj.weight', 'esmmodel.layers.15.self_attn.out_proj.bias', 'esmmodel.layers.15.self_attn.rot_emb.inv_freq', 'esmmodel.layers.15.self_attn_layer_norm.weight', 'esmmodel.layers.15.self_attn_layer_norm.bias', 'esmmodel.layers.15.fc1.weight', 'esmmodel.layers.15.fc1.bias', 'esmmodel.layers.15.fc2.weight', 'esmmodel.layers.15.fc2.bias', 'esmmodel.layers.15.final_layer_norm.weight', 'esmmodel.layers.15.final_layer_norm.bias', 'esmmodel.layers.16.self_attn.k_proj.weight', 'esmmodel.layers.16.self_attn.k_proj.bias', 'esmmodel.layers.16.self_attn.v_proj.weight', 'esmmodel.layers.16.self_attn.v_proj.bias', 'esmmodel.layers.16.self_attn.q_proj.weight', 'esmmodel.layers.16.self_attn.q_proj.bias', 'esmmodel.layers.16.self_attn.out_proj.weight', 'esmmodel.layers.16.self_attn.out_proj.bias', 'esmmodel.layers.16.self_attn.rot_emb.inv_freq', 'esmmodel.layers.16.self_attn_layer_norm.weight', 'esmmodel.layers.16.self_attn_layer_norm.bias', 'esmmodel.layers.16.fc1.weight', 'esmmodel.layers.16.fc1.bias', 'esmmodel.layers.16.fc2.weight', 'esmmodel.layers.16.fc2.bias', 'esmmodel.layers.16.final_layer_norm.weight', 'esmmodel.layers.16.final_layer_norm.bias', 'esmmodel.layers.17.self_attn.k_proj.weight', 'esmmodel.layers.17.self_attn.k_proj.bias', 'esmmodel.layers.17.self_attn.v_proj.weight', 'esmmodel.layers.17.self_attn.v_proj.bias', 'esmmodel.layers.17.self_attn.q_proj.weight', 'esmmodel.layers.17.self_attn.q_proj.bias', 'esmmodel.layers.17.self_attn.out_proj.weight', 'esmmodel.layers.17.self_attn.out_proj.bias', 'esmmodel.layers.17.self_attn.rot_emb.inv_freq', 'esmmodel.layers.17.self_attn_layer_norm.weight', 'esmmodel.layers.17.self_attn_layer_norm.bias', 'esmmodel.layers.17.fc1.weight', 'esmmodel.layers.17.fc1.bias', 'esmmodel.layers.17.fc2.weight', 'esmmodel.layers.17.fc2.bias', 'esmmodel.layers.17.final_layer_norm.weight', 'esmmodel.layers.17.final_layer_norm.bias', 'esmmodel.layers.18.self_attn.k_proj.weight', 'esmmodel.layers.18.self_attn.k_proj.bias', 'esmmodel.layers.18.self_attn.v_proj.weight', 'esmmodel.layers.18.self_attn.v_proj.bias', 'esmmodel.layers.18.self_attn.q_proj.weight', 'esmmodel.layers.18.self_attn.q_proj.bias', 'esmmodel.layers.18.self_attn.out_proj.weight', 'esmmodel.layers.18.self_attn.out_proj.bias', 'esmmodel.layers.18.self_attn.rot_emb.inv_freq', 'esmmodel.layers.18.self_attn_layer_norm.weight', 'esmmodel.layers.18.self_attn_layer_norm.bias', 'esmmodel.layers.18.fc1.weight', 'esmmodel.layers.18.fc1.bias', 'esmmodel.layers.18.fc2.weight', 'esmmodel.layers.18.fc2.bias', 'esmmodel.layers.18.final_layer_norm.weight', 'esmmodel.layers.18.final_layer_norm.bias', 'esmmodel.layers.19.self_attn.k_proj.weight', 'esmmodel.layers.19.self_attn.k_proj.bias', 'esmmodel.layers.19.self_attn.v_proj.weight', 'esmmodel.layers.19.self_attn.v_proj.bias', 'esmmodel.layers.19.self_attn.q_proj.weight', 'esmmodel.layers.19.self_attn.q_proj.bias', 'esmmodel.layers.19.self_attn.out_proj.weight', 'esmmodel.layers.19.self_attn.out_proj.bias', 'esmmodel.layers.19.self_attn.rot_emb.inv_freq', 'esmmodel.layers.19.self_attn_layer_norm.weight', 'esmmodel.layers.19.self_attn_layer_norm.bias', 'esmmodel.layers.19.fc1.weight', 'esmmodel.layers.19.fc1.bias', 'esmmodel.layers.19.fc2.weight', 'esmmodel.layers.19.fc2.bias', 'esmmodel.layers.19.final_layer_norm.weight', 'esmmodel.layers.19.final_layer_norm.bias', 'esmmodel.layers.20.self_attn.k_proj.weight', 'esmmodel.layers.20.self_attn.k_proj.bias', 'esmmodel.layers.20.self_attn.v_proj.weight', 'esmmodel.layers.20.self_attn.v_proj.bias', 'esmmodel.layers.20.self_attn.q_proj.weight', 'esmmodel.layers.20.self_attn.q_proj.bias', 'esmmodel.layers.20.self_attn.out_proj.weight', 'esmmodel.layers.20.self_attn.out_proj.bias', 'esmmodel.layers.20.self_attn.rot_emb.inv_freq', 'esmmodel.layers.20.self_attn_layer_norm.weight', 'esmmodel.layers.20.self_attn_layer_norm.bias', 'esmmodel.layers.20.fc1.weight', 'esmmodel.layers.20.fc1.bias', 'esmmodel.layers.20.fc2.weight', 'esmmodel.layers.20.fc2.bias', 'esmmodel.layers.20.final_layer_norm.weight', 'esmmodel.layers.20.final_layer_norm.bias', 'esmmodel.layers.21.self_attn.k_proj.weight', 'esmmodel.layers.21.self_attn.k_proj.bias', 'esmmodel.layers.21.self_attn.v_proj.weight', 'esmmodel.layers.21.self_attn.v_proj.bias', 'esmmodel.layers.21.self_attn.q_proj.weight', 'esmmodel.layers.21.self_attn.q_proj.bias', 'esmmodel.layers.21.self_attn.out_proj.weight', 'esmmodel.layers.21.self_attn.out_proj.bias', 'esmmodel.layers.21.self_attn.rot_emb.inv_freq', 'esmmodel.layers.21.self_attn_layer_norm.weight', 'esmmodel.layers.21.self_attn_layer_norm.bias', 'esmmodel.layers.21.fc1.weight', 'esmmodel.layers.21.fc1.bias', 'esmmodel.layers.21.fc2.weight', 'esmmodel.layers.21.fc2.bias', 'esmmodel.layers.21.final_layer_norm.weight', 'esmmodel.layers.21.final_layer_norm.bias', 'esmmodel.layers.22.self_attn.k_proj.weight', 'esmmodel.layers.22.self_attn.k_proj.bias', 'esmmodel.layers.22.self_attn.v_proj.weight', 'esmmodel.layers.22.self_attn.v_proj.bias', 'esmmodel.layers.22.self_attn.q_proj.weight', 'esmmodel.layers.22.self_attn.q_proj.bias', 'esmmodel.layers.22.self_attn.out_proj.weight', 'esmmodel.layers.22.self_attn.out_proj.bias', 'esmmodel.layers.22.self_attn.rot_emb.inv_freq', 'esmmodel.layers.22.self_attn_layer_norm.weight', 'esmmodel.layers.22.self_attn_layer_norm.bias', 'esmmodel.layers.22.fc1.weight', 'esmmodel.layers.22.fc1.bias', 'esmmodel.layers.22.fc2.weight', 'esmmodel.layers.22.fc2.bias', 'esmmodel.layers.22.final_layer_norm.weight', 'esmmodel.layers.22.final_layer_norm.bias', 'esmmodel.layers.23.self_attn.k_proj.weight', 'esmmodel.layers.23.self_attn.k_proj.bias', 'esmmodel.layers.23.self_attn.v_proj.weight', 'esmmodel.layers.23.self_attn.v_proj.bias', 'esmmodel.layers.23.self_attn.q_proj.weight', 'esmmodel.layers.23.self_attn.q_proj.bias', 'esmmodel.layers.23.self_attn.out_proj.weight', 'esmmodel.layers.23.self_attn.out_proj.bias', 'esmmodel.layers.23.self_attn.rot_emb.inv_freq', 'esmmodel.layers.23.self_attn_layer_norm.weight', 'esmmodel.layers.23.self_attn_layer_norm.bias', 'esmmodel.layers.23.fc1.weight', 'esmmodel.layers.23.fc1.bias', 'esmmodel.layers.23.fc2.weight', 'esmmodel.layers.23.fc2.bias', 'esmmodel.layers.23.final_layer_norm.weight', 'esmmodel.layers.23.final_layer_norm.bias', 'esmmodel.layers.24.self_attn.k_proj.weight', 'esmmodel.layers.24.self_attn.k_proj.bias', 'esmmodel.layers.24.self_attn.v_proj.weight', 'esmmodel.layers.24.self_attn.v_proj.bias', 'esmmodel.layers.24.self_attn.q_proj.weight', 'esmmodel.layers.24.self_attn.q_proj.bias', 'esmmodel.layers.24.self_attn.out_proj.weight', 'esmmodel.layers.24.self_attn.out_proj.bias', 'esmmodel.layers.24.self_attn.rot_emb.inv_freq', 'esmmodel.layers.24.self_attn_layer_norm.weight', 'esmmodel.layers.24.self_attn_layer_norm.bias', 'esmmodel.layers.24.fc1.weight', 'esmmodel.layers.24.fc1.bias', 'esmmodel.layers.24.fc2.weight', 'esmmodel.layers.24.fc2.bias', 'esmmodel.layers.24.final_layer_norm.weight', 'esmmodel.layers.24.final_layer_norm.bias', 'esmmodel.layers.25.self_attn.k_proj.weight', 'esmmodel.layers.25.self_attn.k_proj.bias', 'esmmodel.layers.25.self_attn.v_proj.weight', 'esmmodel.layers.25.self_attn.v_proj.bias', 'esmmodel.layers.25.self_attn.q_proj.weight', 'esmmodel.layers.25.self_attn.q_proj.bias', 'esmmodel.layers.25.self_attn.out_proj.weight', 'esmmodel.layers.25.self_attn.out_proj.bias', 'esmmodel.layers.25.self_attn.rot_emb.inv_freq', 'esmmodel.layers.25.self_attn_layer_norm.weight', 'esmmodel.layers.25.self_attn_layer_norm.bias', 'esmmodel.layers.25.fc1.weight', 'esmmodel.layers.25.fc1.bias', 'esmmodel.layers.25.fc2.weight', 'esmmodel.layers.25.fc2.bias', 'esmmodel.layers.25.final_layer_norm.weight', 'esmmodel.layers.25.final_layer_norm.bias', 'esmmodel.layers.26.self_attn.k_proj.weight', 'esmmodel.layers.26.self_attn.k_proj.bias', 'esmmodel.layers.26.self_attn.v_proj.weight', 'esmmodel.layers.26.self_attn.v_proj.bias', 'esmmodel.layers.26.self_attn.q_proj.weight', 'esmmodel.layers.26.self_attn.q_proj.bias', 'esmmodel.layers.26.self_attn.out_proj.weight', 'esmmodel.layers.26.self_attn.out_proj.bias', 'esmmodel.layers.26.self_attn.rot_emb.inv_freq', 'esmmodel.layers.26.self_attn_layer_norm.weight', 'esmmodel.layers.26.self_attn_layer_norm.bias', 'esmmodel.layers.26.fc1.weight', 'esmmodel.layers.26.fc1.bias', 'esmmodel.layers.26.fc2.weight', 'esmmodel.layers.26.fc2.bias', 'esmmodel.layers.26.final_layer_norm.weight', 'esmmodel.layers.26.final_layer_norm.bias', 'esmmodel.layers.27.self_attn.k_proj.weight', 'esmmodel.layers.27.self_attn.k_proj.bias', 'esmmodel.layers.27.self_attn.v_proj.weight', 'esmmodel.layers.27.self_attn.v_proj.bias', 'esmmodel.layers.27.self_attn.q_proj.weight', 'esmmodel.layers.27.self_attn.q_proj.bias', 'esmmodel.layers.27.self_attn.out_proj.weight', 'esmmodel.layers.27.self_attn.out_proj.bias', 'esmmodel.layers.27.self_attn.rot_emb.inv_freq', 'esmmodel.layers.27.self_attn_layer_norm.weight', 'esmmodel.layers.27.self_attn_layer_norm.bias', 'esmmodel.layers.27.fc1.weight', 'esmmodel.layers.27.fc1.bias', 'esmmodel.layers.27.fc2.weight', 'esmmodel.layers.27.fc2.bias', 'esmmodel.layers.27.final_layer_norm.weight', 'esmmodel.layers.27.final_layer_norm.bias', 'esmmodel.layers.28.self_attn.k_proj.weight', 'esmmodel.layers.28.self_attn.k_proj.bias', 'esmmodel.layers.28.self_attn.v_proj.weight', 'esmmodel.layers.28.self_attn.v_proj.bias', 'esmmodel.layers.28.self_attn.q_proj.weight', 'esmmodel.layers.28.self_attn.q_proj.bias', 'esmmodel.layers.28.self_attn.out_proj.weight', 'esmmodel.layers.28.self_attn.out_proj.bias', 'esmmodel.layers.28.self_attn.rot_emb.inv_freq', 'esmmodel.layers.28.self_attn_layer_norm.weight', 'esmmodel.layers.28.self_attn_layer_norm.bias', 'esmmodel.layers.28.fc1.weight', 'esmmodel.layers.28.fc1.bias', 'esmmodel.layers.28.fc2.weight', 'esmmodel.layers.28.fc2.bias', 'esmmodel.layers.28.final_layer_norm.weight', 'esmmodel.layers.28.final_layer_norm.bias', 'esmmodel.layers.29.self_attn.k_proj.weight', 'esmmodel.layers.29.self_attn.k_proj.bias', 'esmmodel.layers.29.self_attn.v_proj.weight', 'esmmodel.layers.29.self_attn.v_proj.bias', 'esmmodel.layers.29.self_attn.q_proj.weight', 'esmmodel.layers.29.self_attn.q_proj.bias', 'esmmodel.layers.29.self_attn.out_proj.weight', 'esmmodel.layers.29.self_attn.out_proj.bias', 'esmmodel.layers.29.self_attn.rot_emb.inv_freq', 'esmmodel.layers.29.self_attn_layer_norm.weight', 'esmmodel.layers.29.self_attn_layer_norm.bias', 'esmmodel.layers.29.fc1.weight', 'esmmodel.layers.29.fc1.bias', 'esmmodel.layers.29.fc2.weight', 'esmmodel.layers.29.fc2.bias', 'esmmodel.layers.29.final_layer_norm.weight', 'esmmodel.layers.29.final_layer_norm.bias', 'esmmodel.layers.30.self_attn.k_proj.weight', 'esmmodel.layers.30.self_attn.k_proj.bias', 'esmmodel.layers.30.self_attn.v_proj.weight', 'esmmodel.layers.30.self_attn.v_proj.bias', 'esmmodel.layers.30.self_attn.q_proj.weight', 'esmmodel.layers.30.self_attn.q_proj.bias', 'esmmodel.layers.30.self_attn.out_proj.weight', 'esmmodel.layers.30.self_attn.out_proj.bias', 'esmmodel.layers.30.self_attn.rot_emb.inv_freq', 'esmmodel.layers.30.self_attn_layer_norm.weight', 'esmmodel.layers.30.self_attn_layer_norm.bias', 'esmmodel.layers.30.fc1.weight', 'esmmodel.layers.30.fc1.bias', 'esmmodel.layers.30.fc2.weight', 'esmmodel.layers.30.fc2.bias', 'esmmodel.layers.30.final_layer_norm.weight', 'esmmodel.layers.30.final_layer_norm.bias', 'esmmodel.layers.31.self_attn.k_proj.weight', 'esmmodel.layers.31.self_attn.k_proj.bias', 'esmmodel.layers.31.self_attn.v_proj.weight', 'esmmodel.layers.31.self_attn.v_proj.bias', 'esmmodel.layers.31.self_attn.q_proj.weight', 'esmmodel.layers.31.self_attn.q_proj.bias', 'esmmodel.layers.31.self_attn.out_proj.weight', 'esmmodel.layers.31.self_attn.out_proj.bias', 'esmmodel.layers.31.self_attn.rot_emb.inv_freq', 'esmmodel.layers.31.self_attn_layer_norm.weight', 'esmmodel.layers.31.self_attn_layer_norm.bias', 'esmmodel.layers.31.fc1.weight', 'esmmodel.layers.31.fc1.bias', 'esmmodel.layers.31.fc2.weight', 'esmmodel.layers.31.fc2.bias', 'esmmodel.layers.31.final_layer_norm.weight', 'esmmodel.layers.31.final_layer_norm.bias', 'esmmodel.layers.32.self_attn.k_proj.weight', 'esmmodel.layers.32.self_attn.k_proj.bias', 'esmmodel.layers.32.self_attn.v_proj.weight', 'esmmodel.layers.32.self_attn.v_proj.bias', 'esmmodel.layers.32.self_attn.q_proj.weight', 'esmmodel.layers.32.self_attn.q_proj.bias', 'esmmodel.layers.32.self_attn.out_proj.weight', 'esmmodel.layers.32.self_attn.out_proj.bias', 'esmmodel.layers.32.self_attn.rot_emb.inv_freq', 'esmmodel.layers.32.self_attn_layer_norm.weight', 'esmmodel.layers.32.self_attn_layer_norm.bias', 'esmmodel.layers.32.fc1.weight', 'esmmodel.layers.32.fc1.bias', 'esmmodel.layers.32.fc2.weight', 'esmmodel.layers.32.fc2.bias', 'esmmodel.layers.32.final_layer_norm.weight', 'esmmodel.layers.32.final_layer_norm.bias', 'esmmodel.contact_head.regression.weight', 'esmmodel.contact_head.regression.bias', 'esmmodel.emb_layer_norm_after.weight', 'esmmodel.emb_layer_norm_after.bias', 'esmmodel.lm_head.weight', 'esmmodel.lm_head.bias', 'esmmodel.lm_head.dense.weight', 'esmmodel.lm_head.dense.bias', 'esmmodel.lm_head.layer_norm.weight', 'esmmodel.lm_head.layer_norm.bias']\n"
     ]
    }
   ],
   "source": [
    "ltraygun = RaygunLightning.load_from_checkpoint(checkpoint,\n",
    "                                                raygun=raymodel,\n",
    "                                               esmmodel=emodel,\n",
    "                                               strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from Bio import SeqIO\n",
    "\n",
    "gfprec = \"\"\">sp|P42212|GFP_AEQVI Green fluorescent protein OS=Aequorea victoria OX=6100 GN=GFP PE=1 SV=1\n",
    "MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTL\n",
    "VTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLV\n",
    "NRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLAD\n",
    "HYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\n",
    "\"\"\"\n",
    "gfpseq = str(SeqIO.read(StringIO(gfprec), \"fasta\").seq)\n",
    "len(gfpseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, tok = ealph.get_batch_converter()([(\"gfp\", gfpseq)])\n",
    "emodel    = emodel.eval().to(0)\n",
    "embed     = emodel.forward(tok.to(0), return_contacts=False, repr_layers=[33])[\"representations\"][33][:, 1:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltraymodel = ltraygun.model.eval().to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor type unknown to einops <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m payload = \u001b[43mltraymodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_logits_and_seqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/raygun/raygun/modelv3/raygun.py:218\u001b[39m, in \u001b[36mRaygun.forward\u001b[39m\u001b[34m(self, x, mask, target_lengths, noise, token, return_logits_and_seqs, temperature, include_valid_only)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mbatch larger than 1 but mask is Null\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m     lengths = mask.sum(dim = -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m mem = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m out = \u001b[38;5;28mself\u001b[39m.decoder(mem, lengths)\n\u001b[32m    221\u001b[39m result = {\u001b[33m\"\u001b[39m\u001b[33mfixed_length_embedding\u001b[39m\u001b[33m\"\u001b[39m: mem, \n\u001b[32m    222\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mreconstructed_embedding\u001b[39m\u001b[33m\"\u001b[39m: out}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/raygun/raygun/modelv3/raygun.py:71\u001b[39m, in \u001b[36mRaygunEncoder.forward\u001b[39m\u001b[34m(self, x, mask, noise, add_at_first_only)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoders:\n\u001b[32m     70\u001b[39m     xresidue = mod(x, mask = mask)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     residue  = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxresidue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madd_at_first_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     73\u001b[39m     x        = x + xresidue\n\u001b[32m     74\u001b[39m     residues.append(residue)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/raygun/raygun/modelv3/model_utils.py:67\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     x    = \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb n c -> n b c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     x, _ = \u001b[38;5;28mself\u001b[39m.encoder(x, self_attn_padding_mask = ~mask \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m mask) \n\u001b[32m     69\u001b[39m     x    = rearrange(x, \u001b[33m\"\u001b[39m\u001b[33mn b c -> b n c\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/einops/einops.py:600\u001b[39m, in \u001b[36mrearrange\u001b[39m\u001b[34m(tensor, pattern, **axes_lengths)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, **axes_lengths: Size) -> Tensor:\n\u001b[32m    546\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \n\u001b[32m    599\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrearrange\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/einops/einops.py:527\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    525\u001b[39m     tensor = backend.stack_on_zeroth_dimension(tensor)\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     backend = \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m hashable_axes_lengths = \u001b[38;5;28mtuple\u001b[39m(axes_lengths.items())\n\u001b[32m    530\u001b[39m shape = backend.shape(tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/singhlab/user/rs670/micromamba/envs/raygun/lib/python3.11/site-packages/einops/_backends.py:59\u001b[39m, in \u001b[36mget_backend\u001b[39m\u001b[34m(tensor)\u001b[39m\n\u001b[32m     56\u001b[39m                 _type2backend[_type] = backend\n\u001b[32m     57\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor type unknown to einops \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mtype\u001b[39m(tensor)))\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensor type unknown to einops <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "payload = ltraymodel.forward(embed, return_logits_and_seqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raygun",
   "language": "python",
   "name": "raygun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
